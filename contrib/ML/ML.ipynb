{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-14T00:35:37.426325Z",
     "start_time": "2021-02-14T00:35:35.733226Z"
    }
   },
   "outputs": [],
   "source": [
    "from data import ReadData\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-14T00:35:37.764486Z",
     "start_time": "2021-02-14T00:35:37.427842Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 4) 4\n",
      "0                                                    1\n",
      "1    -0.000000_0.000000_0.000000_0.000000_0.000000_...\n",
      "2    0.000000_0.000000_0.000000_0.000000_0.000000_0...\n",
      "3    0.000000_0.000000_2218.760498_1875.834106_1179...\n",
      "Name: 0, dtype: object\n",
      "(60, 12)\n",
      "Index(['ObjCoeff', 'yLP', 'RC', 'ylp_std', 'ylp_ema', 'y_0', 'y_1', 'dual_std',\n",
      "       'dual_ema', 'dual_0', 'rc_std', 'rc_0'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ObjCoeff</th>\n",
       "      <th>yLP</th>\n",
       "      <th>RC</th>\n",
       "      <th>ylp_std</th>\n",
       "      <th>ylp_ema</th>\n",
       "      <th>y_0</th>\n",
       "      <th>y_1</th>\n",
       "      <th>dual_std</th>\n",
       "      <th>dual_ema</th>\n",
       "      <th>dual_0</th>\n",
       "      <th>rc_std</th>\n",
       "      <th>rc_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.568663</td>\n",
       "      <td>-0.680998</td>\n",
       "      <td>-0.215768</td>\n",
       "      <td>-0.466025</td>\n",
       "      <td>1.224893e-12</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.664010</td>\n",
       "      <td>0.735678</td>\n",
       "      <td>1.421505</td>\n",
       "      <td>1.725125</td>\n",
       "      <td>-0.444962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.627870</td>\n",
       "      <td>-0.677595</td>\n",
       "      <td>-0.382525</td>\n",
       "      <td>-0.371610</td>\n",
       "      <td>1.539765e-13</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.659058</td>\n",
       "      <td>0.715266</td>\n",
       "      <td>0.188488</td>\n",
       "      <td>0.014873</td>\n",
       "      <td>-0.444962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.191491</td>\n",
       "      <td>-0.682744</td>\n",
       "      <td>-0.559587</td>\n",
       "      <td>-0.535012</td>\n",
       "      <td>6.116775e-05</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.406684</td>\n",
       "      <td>0.401936</td>\n",
       "      <td>-0.039848</td>\n",
       "      <td>-0.608326</td>\n",
       "      <td>-0.444962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.154303</td>\n",
       "      <td>-0.500718</td>\n",
       "      <td>-0.574337</td>\n",
       "      <td>0.479347</td>\n",
       "      <td>1.467191e-01</td>\n",
       "      <td>0.981818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.069888</td>\n",
       "      <td>-3.996999</td>\n",
       "      <td>-0.679190</td>\n",
       "      <td>-0.770071</td>\n",
       "      <td>-0.444962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.627870</td>\n",
       "      <td>1.763695</td>\n",
       "      <td>-0.840518</td>\n",
       "      <td>-0.548335</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.134165</td>\n",
       "      <td>0.524444</td>\n",
       "      <td>-0.359519</td>\n",
       "      <td>0.637014</td>\n",
       "      <td>3.579112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.414614</td>\n",
       "      <td>-0.681394</td>\n",
       "      <td>-0.234473</td>\n",
       "      <td>-0.500670</td>\n",
       "      <td>1.183359e-08</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.477598</td>\n",
       "      <td>0.594859</td>\n",
       "      <td>0.690828</td>\n",
       "      <td>0.924593</td>\n",
       "      <td>-0.444962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.451801</td>\n",
       "      <td>-0.675461</td>\n",
       "      <td>-0.309409</td>\n",
       "      <td>-0.444876</td>\n",
       "      <td>2.690520e-04</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.597154</td>\n",
       "      <td>0.735678</td>\n",
       "      <td>0.827830</td>\n",
       "      <td>0.730450</td>\n",
       "      <td>-0.444962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.046801</td>\n",
       "      <td>-0.680763</td>\n",
       "      <td>-0.147141</td>\n",
       "      <td>-0.501349</td>\n",
       "      <td>6.160080e-09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.642175</td>\n",
       "      <td>0.295497</td>\n",
       "      <td>0.234155</td>\n",
       "      <td>1.259253</td>\n",
       "      <td>-0.444962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1.669116</td>\n",
       "      <td>-0.624846</td>\n",
       "      <td>-0.575442</td>\n",
       "      <td>0.669024</td>\n",
       "      <td>8.394544e-04</td>\n",
       "      <td>0.981818</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>-0.488375</td>\n",
       "      <td>0.734486</td>\n",
       "      <td>0.873497</td>\n",
       "      <td>-0.690961</td>\n",
       "      <td>-0.355538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.568663</td>\n",
       "      <td>-0.681264</td>\n",
       "      <td>-0.215850</td>\n",
       "      <td>-0.473424</td>\n",
       "      <td>5.689726e-13</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.685796</td>\n",
       "      <td>0.076757</td>\n",
       "      <td>1.330170</td>\n",
       "      <td>1.785764</td>\n",
       "      <td>-0.444962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.600554</td>\n",
       "      <td>1.180390</td>\n",
       "      <td>-0.609104</td>\n",
       "      <td>2.628378</td>\n",
       "      <td>9.999525e-01</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.672727</td>\n",
       "      <td>0.828594</td>\n",
       "      <td>0.724277</td>\n",
       "      <td>-0.359519</td>\n",
       "      <td>-0.297839</td>\n",
       "      <td>0.181005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.031633</td>\n",
       "      <td>-0.683969</td>\n",
       "      <td>-0.279936</td>\n",
       "      <td>-0.552373</td>\n",
       "      <td>1.378320e-05</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.598531</td>\n",
       "      <td>-0.133278</td>\n",
       "      <td>1.056167</td>\n",
       "      <td>0.312410</td>\n",
       "      <td>-0.444962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-1.036930</td>\n",
       "      <td>1.640587</td>\n",
       "      <td>-0.583022</td>\n",
       "      <td>0.890474</td>\n",
       "      <td>9.996283e-01</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>-0.130091</td>\n",
       "      <td>0.385536</td>\n",
       "      <td>0.188488</td>\n",
       "      <td>-0.666495</td>\n",
       "      <td>0.270429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.117116</td>\n",
       "      <td>-0.626809</td>\n",
       "      <td>-0.474509</td>\n",
       "      <td>0.636525</td>\n",
       "      <td>2.451660e-06</td>\n",
       "      <td>0.963636</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>-0.259993</td>\n",
       "      <td>0.735596</td>\n",
       "      <td>0.142821</td>\n",
       "      <td>-0.281769</td>\n",
       "      <td>-0.444962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.046801</td>\n",
       "      <td>-0.684632</td>\n",
       "      <td>-0.181505</td>\n",
       "      <td>-0.567466</td>\n",
       "      <td>3.193750e-04</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.221457</td>\n",
       "      <td>-0.346267</td>\n",
       "      <td>-0.176850</td>\n",
       "      <td>1.089641</td>\n",
       "      <td>-0.444962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.568663</td>\n",
       "      <td>-0.672939</td>\n",
       "      <td>-0.576024</td>\n",
       "      <td>-0.395419</td>\n",
       "      <td>5.788399e-07</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.235493</td>\n",
       "      <td>0.733158</td>\n",
       "      <td>0.234155</td>\n",
       "      <td>-0.805802</td>\n",
       "      <td>-0.444962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.308352</td>\n",
       "      <td>-0.170104</td>\n",
       "      <td>-0.581121</td>\n",
       "      <td>2.373183</td>\n",
       "      <td>8.210315e-01</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.163636</td>\n",
       "      <td>0.846804</td>\n",
       "      <td>-0.416943</td>\n",
       "      <td>-0.679190</td>\n",
       "      <td>-0.674427</td>\n",
       "      <td>-0.266114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-1.260056</td>\n",
       "      <td>-0.041634</td>\n",
       "      <td>-0.580438</td>\n",
       "      <td>2.783217</td>\n",
       "      <td>2.533065e-04</td>\n",
       "      <td>0.654545</td>\n",
       "      <td>0.218182</td>\n",
       "      <td>-0.189325</td>\n",
       "      <td>0.731679</td>\n",
       "      <td>0.325490</td>\n",
       "      <td>-0.653983</td>\n",
       "      <td>-0.355538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.665057</td>\n",
       "      <td>0.748924</td>\n",
       "      <td>-0.576305</td>\n",
       "      <td>2.601193</td>\n",
       "      <td>1.787345e-01</td>\n",
       "      <td>0.309091</td>\n",
       "      <td>0.527273</td>\n",
       "      <td>-0.056810</td>\n",
       "      <td>0.268177</td>\n",
       "      <td>0.188488</td>\n",
       "      <td>-0.797343</td>\n",
       "      <td>-0.355538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.085229</td>\n",
       "      <td>-0.679743</td>\n",
       "      <td>-0.405342</td>\n",
       "      <td>-0.431370</td>\n",
       "      <td>1.435547e-07</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.139028</td>\n",
       "      <td>-1.401283</td>\n",
       "      <td>-0.587856</td>\n",
       "      <td>0.194598</td>\n",
       "      <td>-0.444962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-1.706303</td>\n",
       "      <td>-0.336457</td>\n",
       "      <td>-0.587217</td>\n",
       "      <td>2.480665</td>\n",
       "      <td>1.179124e-01</td>\n",
       "      <td>0.872727</td>\n",
       "      <td>0.127273</td>\n",
       "      <td>-0.287491</td>\n",
       "      <td>0.706219</td>\n",
       "      <td>0.782163</td>\n",
       "      <td>-0.497475</td>\n",
       "      <td>0.181005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.568663</td>\n",
       "      <td>-0.682550</td>\n",
       "      <td>-0.233150</td>\n",
       "      <td>-0.509175</td>\n",
       "      <td>8.972711e-14</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.734320</td>\n",
       "      <td>0.614111</td>\n",
       "      <td>1.147501</td>\n",
       "      <td>1.764267</td>\n",
       "      <td>-0.444962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-1.483179</td>\n",
       "      <td>-0.678894</td>\n",
       "      <td>-0.576024</td>\n",
       "      <td>-0.407533</td>\n",
       "      <td>2.390316e-13</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.695045</td>\n",
       "      <td>0.150052</td>\n",
       "      <td>1.330170</td>\n",
       "      <td>-0.805802</td>\n",
       "      <td>-0.444962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.143195</td>\n",
       "      <td>-0.684746</td>\n",
       "      <td>-0.020283</td>\n",
       "      <td>-0.570258</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.378471</td>\n",
       "      <td>-0.382737</td>\n",
       "      <td>1.010499</td>\n",
       "      <td>0.735819</td>\n",
       "      <td>-0.444962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.329131</td>\n",
       "      <td>1.149394</td>\n",
       "      <td>-0.576931</td>\n",
       "      <td>2.737095</td>\n",
       "      <td>9.999256e-01</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.672727</td>\n",
       "      <td>-0.422657</td>\n",
       "      <td>0.075197</td>\n",
       "      <td>0.325490</td>\n",
       "      <td>-0.500047</td>\n",
       "      <td>0.002157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.196791</td>\n",
       "      <td>-0.606715</td>\n",
       "      <td>-0.546277</td>\n",
       "      <td>0.266203</td>\n",
       "      <td>5.849801e-07</td>\n",
       "      <td>0.963636</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.100033</td>\n",
       "      <td>-2.417746</td>\n",
       "      <td>-1.044529</td>\n",
       "      <td>-0.545321</td>\n",
       "      <td>-0.444962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.526179</td>\n",
       "      <td>-0.676976</td>\n",
       "      <td>-0.450230</td>\n",
       "      <td>-0.419232</td>\n",
       "      <td>3.174273e-11</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.088485</td>\n",
       "      <td>0.130518</td>\n",
       "      <td>-0.587856</td>\n",
       "      <td>0.075257</td>\n",
       "      <td>-0.444962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.823677</td>\n",
       "      <td>-0.664523</td>\n",
       "      <td>-0.570107</td>\n",
       "      <td>-0.268002</td>\n",
       "      <td>9.946460e-06</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.104682</td>\n",
       "      <td>-2.297270</td>\n",
       "      <td>-1.044529</td>\n",
       "      <td>-0.708015</td>\n",
       "      <td>-0.444962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.568663</td>\n",
       "      <td>-0.682455</td>\n",
       "      <td>0.049499</td>\n",
       "      <td>-0.519396</td>\n",
       "      <td>5.438233e-08</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.366743</td>\n",
       "      <td>0.255881</td>\n",
       "      <td>-0.359519</td>\n",
       "      <td>1.102030</td>\n",
       "      <td>-0.444962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-1.371618</td>\n",
       "      <td>1.689451</td>\n",
       "      <td>-0.600291</td>\n",
       "      <td>0.368068</td>\n",
       "      <td>9.999998e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.945455</td>\n",
       "      <td>-0.517281</td>\n",
       "      <td>-0.369714</td>\n",
       "      <td>-0.222518</td>\n",
       "      <td>-0.470535</td>\n",
       "      <td>1.164668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.196791</td>\n",
       "      <td>-0.137434</td>\n",
       "      <td>-0.570657</td>\n",
       "      <td>2.547983</td>\n",
       "      <td>1.074890e-05</td>\n",
       "      <td>0.690909</td>\n",
       "      <td>0.163636</td>\n",
       "      <td>0.863703</td>\n",
       "      <td>-0.528107</td>\n",
       "      <td>-0.222518</td>\n",
       "      <td>-0.679506</td>\n",
       "      <td>-0.355538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.122416</td>\n",
       "      <td>-0.684746</td>\n",
       "      <td>-0.211403</td>\n",
       "      <td>-0.570258</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.899096</td>\n",
       "      <td>-0.287204</td>\n",
       "      <td>-0.770525</td>\n",
       "      <td>0.242584</td>\n",
       "      <td>-0.444962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.457101</td>\n",
       "      <td>-0.574255</td>\n",
       "      <td>-0.534991</td>\n",
       "      <td>1.179658</td>\n",
       "      <td>7.844506e-16</td>\n",
       "      <td>0.945455</td>\n",
       "      <td>0.036364</td>\n",
       "      <td>0.412787</td>\n",
       "      <td>0.238580</td>\n",
       "      <td>-0.724858</td>\n",
       "      <td>-0.303842</td>\n",
       "      <td>-0.355538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.568663</td>\n",
       "      <td>-0.679499</td>\n",
       "      <td>-0.275722</td>\n",
       "      <td>-0.479618</td>\n",
       "      <td>2.465799e-06</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.512731</td>\n",
       "      <td>-0.286933</td>\n",
       "      <td>0.097154</td>\n",
       "      <td>1.428044</td>\n",
       "      <td>-0.444962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-0.254756</td>\n",
       "      <td>-0.678646</td>\n",
       "      <td>-0.402653</td>\n",
       "      <td>-0.400617</td>\n",
       "      <td>3.987050e-12</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.741153</td>\n",
       "      <td>0.735675</td>\n",
       "      <td>1.330170</td>\n",
       "      <td>0.407375</td>\n",
       "      <td>-0.444962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.303052</td>\n",
       "      <td>-0.679430</td>\n",
       "      <td>-0.489293</td>\n",
       "      <td>-0.476418</td>\n",
       "      <td>1.899636e-10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.508248</td>\n",
       "      <td>0.320254</td>\n",
       "      <td>0.416825</td>\n",
       "      <td>-0.290214</td>\n",
       "      <td>-0.444962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-1.594741</td>\n",
       "      <td>-0.228046</td>\n",
       "      <td>-0.590598</td>\n",
       "      <td>2.178357</td>\n",
       "      <td>1.474631e-01</td>\n",
       "      <td>0.854545</td>\n",
       "      <td>0.109091</td>\n",
       "      <td>-0.568528</td>\n",
       "      <td>0.735678</td>\n",
       "      <td>0.827830</td>\n",
       "      <td>-0.522143</td>\n",
       "      <td>0.091581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-1.074117</td>\n",
       "      <td>0.775124</td>\n",
       "      <td>-0.582294</td>\n",
       "      <td>2.672183</td>\n",
       "      <td>1.797087e-01</td>\n",
       "      <td>0.290909</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>-0.290395</td>\n",
       "      <td>-0.863941</td>\n",
       "      <td>-0.176850</td>\n",
       "      <td>-0.680580</td>\n",
       "      <td>-0.176690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1.568663</td>\n",
       "      <td>-0.682225</td>\n",
       "      <td>-0.040759</td>\n",
       "      <td>-0.500159</td>\n",
       "      <td>6.435698e-15</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.444610</td>\n",
       "      <td>0.732704</td>\n",
       "      <td>0.051486</td>\n",
       "      <td>2.062016</td>\n",
       "      <td>-0.444962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-0.366318</td>\n",
       "      <td>-0.684746</td>\n",
       "      <td>0.009482</td>\n",
       "      <td>-0.570258</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.336217</td>\n",
       "      <td>-0.542050</td>\n",
       "      <td>-0.450854</td>\n",
       "      <td>0.675465</td>\n",
       "      <td>-0.444962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1.419914</td>\n",
       "      <td>0.317944</td>\n",
       "      <td>-0.570726</td>\n",
       "      <td>2.506210</td>\n",
       "      <td>8.152693e-01</td>\n",
       "      <td>0.490909</td>\n",
       "      <td>0.290909</td>\n",
       "      <td>0.572834</td>\n",
       "      <td>-0.703202</td>\n",
       "      <td>-0.405187</td>\n",
       "      <td>-0.599089</td>\n",
       "      <td>-0.266114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>-1.594741</td>\n",
       "      <td>0.804143</td>\n",
       "      <td>-0.576197</td>\n",
       "      <td>2.456710</td>\n",
       "      <td>1.847307e-01</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.509091</td>\n",
       "      <td>-0.570279</td>\n",
       "      <td>0.594543</td>\n",
       "      <td>0.553826</td>\n",
       "      <td>-0.801433</td>\n",
       "      <td>-0.266114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.079929</td>\n",
       "      <td>-0.684726</td>\n",
       "      <td>-0.514242</td>\n",
       "      <td>-0.569895</td>\n",
       "      <td>2.426147e-09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.187780</td>\n",
       "      <td>-0.414871</td>\n",
       "      <td>-1.044529</td>\n",
       "      <td>-0.308557</td>\n",
       "      <td>-0.444962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1.568663</td>\n",
       "      <td>-0.681422</td>\n",
       "      <td>-0.228023</td>\n",
       "      <td>-0.477955</td>\n",
       "      <td>2.752713e-13</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.632404</td>\n",
       "      <td>0.735678</td>\n",
       "      <td>0.827830</td>\n",
       "      <td>1.769331</td>\n",
       "      <td>-0.444962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>-0.031633</td>\n",
       "      <td>0.340540</td>\n",
       "      <td>-0.576024</td>\n",
       "      <td>1.828374</td>\n",
       "      <td>1.771941e-01</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>-0.255284</td>\n",
       "      <td>-0.915400</td>\n",
       "      <td>-0.542189</td>\n",
       "      <td>-0.805802</td>\n",
       "      <td>-0.444962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.079929</td>\n",
       "      <td>-0.675405</td>\n",
       "      <td>-0.494609</td>\n",
       "      <td>-0.452266</td>\n",
       "      <td>4.425918e-05</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.549547</td>\n",
       "      <td>-0.624325</td>\n",
       "      <td>-0.039848</td>\n",
       "      <td>-0.306044</td>\n",
       "      <td>-0.444962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>-0.553495</td>\n",
       "      <td>1.751625</td>\n",
       "      <td>-0.689127</td>\n",
       "      <td>-0.295203</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.158219</td>\n",
       "      <td>0.378628</td>\n",
       "      <td>-0.496521</td>\n",
       "      <td>-0.271886</td>\n",
       "      <td>3.757960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>-0.031633</td>\n",
       "      <td>-0.611859</td>\n",
       "      <td>-0.574941</td>\n",
       "      <td>0.467159</td>\n",
       "      <td>2.321644e-04</td>\n",
       "      <td>0.963636</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.020419</td>\n",
       "      <td>-2.042414</td>\n",
       "      <td>-0.542189</td>\n",
       "      <td>-0.773131</td>\n",
       "      <td>-0.444962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>-0.739432</td>\n",
       "      <td>-0.678477</td>\n",
       "      <td>-0.576024</td>\n",
       "      <td>-0.395924</td>\n",
       "      <td>3.201058e-14</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.590181</td>\n",
       "      <td>0.734015</td>\n",
       "      <td>0.325490</td>\n",
       "      <td>-0.805802</td>\n",
       "      <td>-0.444962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>-0.366318</td>\n",
       "      <td>-0.674336</td>\n",
       "      <td>-0.511830</td>\n",
       "      <td>-0.305414</td>\n",
       "      <td>1.875164e-07</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.371037</td>\n",
       "      <td>-0.505533</td>\n",
       "      <td>-0.176850</td>\n",
       "      <td>-0.085701</td>\n",
       "      <td>-0.444962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>-0.627870</td>\n",
       "      <td>-0.684746</td>\n",
       "      <td>-0.224566</td>\n",
       "      <td>-0.570258</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.628048</td>\n",
       "      <td>0.310364</td>\n",
       "      <td>1.010499</td>\n",
       "      <td>0.379427</td>\n",
       "      <td>-0.444962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1.568663</td>\n",
       "      <td>-0.682091</td>\n",
       "      <td>0.107113</td>\n",
       "      <td>-0.497764</td>\n",
       "      <td>2.924772e-11</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.065142</td>\n",
       "      <td>-1.063512</td>\n",
       "      <td>0.553826</td>\n",
       "      <td>2.457615</td>\n",
       "      <td>-0.444962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>-0.291944</td>\n",
       "      <td>1.755958</td>\n",
       "      <td>-0.753529</td>\n",
       "      <td>-0.430775</td>\n",
       "      <td>9.999962e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.572170</td>\n",
       "      <td>-2.313758</td>\n",
       "      <td>0.188488</td>\n",
       "      <td>0.079588</td>\n",
       "      <td>2.595450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.898052</td>\n",
       "      <td>-0.678945</td>\n",
       "      <td>-0.436401</td>\n",
       "      <td>-0.417220</td>\n",
       "      <td>1.200098e-05</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.377599</td>\n",
       "      <td>-0.441137</td>\n",
       "      <td>-0.861860</td>\n",
       "      <td>0.110516</td>\n",
       "      <td>-0.444962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>-0.813806</td>\n",
       "      <td>0.570014</td>\n",
       "      <td>-0.576024</td>\n",
       "      <td>2.169265</td>\n",
       "      <td>8.225896e-01</td>\n",
       "      <td>0.163636</td>\n",
       "      <td>0.327273</td>\n",
       "      <td>-0.416136</td>\n",
       "      <td>-0.635528</td>\n",
       "      <td>0.416825</td>\n",
       "      <td>-0.805802</td>\n",
       "      <td>-0.444962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.377427</td>\n",
       "      <td>-0.682310</td>\n",
       "      <td>-0.399890</td>\n",
       "      <td>-0.518017</td>\n",
       "      <td>2.136237e-10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.017378</td>\n",
       "      <td>-1.772955</td>\n",
       "      <td>-0.633523</td>\n",
       "      <td>0.005773</td>\n",
       "      <td>-0.444962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1.568663</td>\n",
       "      <td>-0.684746</td>\n",
       "      <td>0.519885</td>\n",
       "      <td>-0.570258</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.222539</td>\n",
       "      <td>0.255655</td>\n",
       "      <td>1.010499</td>\n",
       "      <td>2.303833</td>\n",
       "      <td>-0.444962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.191491</td>\n",
       "      <td>1.763776</td>\n",
       "      <td>-1.072164</td>\n",
       "      <td>-0.550716</td>\n",
       "      <td>9.999995e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.275891</td>\n",
       "      <td>-1.815139</td>\n",
       "      <td>0.051486</td>\n",
       "      <td>0.947975</td>\n",
       "      <td>3.668536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>1.196791</td>\n",
       "      <td>-0.683025</td>\n",
       "      <td>-0.042193</td>\n",
       "      <td>-0.536262</td>\n",
       "      <td>1.021288e-10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.456782</td>\n",
       "      <td>0.148497</td>\n",
       "      <td>0.599494</td>\n",
       "      <td>1.524482</td>\n",
       "      <td>-0.444962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>-0.925368</td>\n",
       "      <td>0.118396</td>\n",
       "      <td>-0.573393</td>\n",
       "      <td>2.750627</td>\n",
       "      <td>6.916352e-01</td>\n",
       "      <td>0.581818</td>\n",
       "      <td>0.254545</td>\n",
       "      <td>-0.301966</td>\n",
       "      <td>-1.808839</td>\n",
       "      <td>0.142821</td>\n",
       "      <td>-0.699456</td>\n",
       "      <td>-0.355538</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ObjCoeff       yLP        RC   ylp_std       ylp_ema       y_0       y_1  \\\n",
       "0   1.568663 -0.680998 -0.215768 -0.466025  1.224893e-12  1.000000  0.000000   \n",
       "1  -0.627870 -0.677595 -0.382525 -0.371610  1.539765e-13  1.000000  0.000000   \n",
       "2   0.191491 -0.682744 -0.559587 -0.535012  6.116775e-05  1.000000  0.000000   \n",
       "3   0.154303 -0.500718 -0.574337  0.479347  1.467191e-01  0.981818  0.000000   \n",
       "4  -0.627870  1.763695 -0.840518 -0.548335  1.000000e+00  0.000000  1.000000   \n",
       "5   0.414614 -0.681394 -0.234473 -0.500670  1.183359e-08  1.000000  0.000000   \n",
       "6   0.451801 -0.675461 -0.309409 -0.444876  2.690520e-04  1.000000  0.000000   \n",
       "7   1.046801 -0.680763 -0.147141 -0.501349  6.160080e-09  1.000000  0.000000   \n",
       "8  -1.669116 -0.624846 -0.575442  0.669024  8.394544e-04  0.981818  0.018182   \n",
       "9   1.568663 -0.681264 -0.215850 -0.473424  5.689726e-13  1.000000  0.000000   \n",
       "10  0.600554  1.180390 -0.609104  2.628378  9.999525e-01  0.200000  0.672727   \n",
       "11 -0.031633 -0.683969 -0.279936 -0.552373  1.378320e-05  1.000000  0.000000   \n",
       "12 -1.036930  1.640587 -0.583022  0.890474  9.996283e-01  0.018182  0.909091   \n",
       "13  0.117116 -0.626809 -0.474509  0.636525  2.451660e-06  0.963636  0.018182   \n",
       "14  1.046801 -0.684632 -0.181505 -0.567466  3.193750e-04  1.000000  0.000000   \n",
       "15  1.568663 -0.672939 -0.576024 -0.395419  5.788399e-07  1.000000  0.000000   \n",
       "16  1.308352 -0.170104 -0.581121  2.373183  8.210315e-01  0.818182  0.163636   \n",
       "17 -1.260056 -0.041634 -0.580438  2.783217  2.533065e-04  0.654545  0.218182   \n",
       "18 -0.665057  0.748924 -0.576305  2.601193  1.787345e-01  0.309091  0.527273   \n",
       "19  1.085229 -0.679743 -0.405342 -0.431370  1.435547e-07  1.000000  0.000000   \n",
       "20 -1.706303 -0.336457 -0.587217  2.480665  1.179124e-01  0.872727  0.127273   \n",
       "21  1.568663 -0.682550 -0.233150 -0.509175  8.972711e-14  1.000000  0.000000   \n",
       "22 -1.483179 -0.678894 -0.576024 -0.407533  2.390316e-13  1.000000  0.000000   \n",
       "23 -0.143195 -0.684746 -0.020283 -0.570258  0.000000e+00  1.000000  0.000000   \n",
       "24 -0.329131  1.149394 -0.576931  2.737095  9.999256e-01  0.200000  0.672727   \n",
       "25  1.196791 -0.606715 -0.546277  0.266203  5.849801e-07  0.963636  0.000000   \n",
       "26  0.526179 -0.676976 -0.450230 -0.419232  3.174273e-11  1.000000  0.000000   \n",
       "27  0.823677 -0.664523 -0.570107 -0.268002  9.946460e-06  1.000000  0.000000   \n",
       "28  1.568663 -0.682455  0.049499 -0.519396  5.438233e-08  1.000000  0.000000   \n",
       "29 -1.371618  1.689451 -0.600291  0.368068  9.999998e-01  0.000000  0.945455   \n",
       "30  1.196791 -0.137434 -0.570657  2.547983  1.074890e-05  0.690909  0.163636   \n",
       "31  1.122416 -0.684746 -0.211403 -0.570258  0.000000e+00  1.000000  0.000000   \n",
       "32  1.457101 -0.574255 -0.534991  1.179658  7.844506e-16  0.945455  0.036364   \n",
       "33  1.568663 -0.679499 -0.275722 -0.479618  2.465799e-06  1.000000  0.000000   \n",
       "34 -0.254756 -0.678646 -0.402653 -0.400617  3.987050e-12  1.000000  0.000000   \n",
       "35  0.303052 -0.679430 -0.489293 -0.476418  1.899636e-10  1.000000  0.000000   \n",
       "36 -1.594741 -0.228046 -0.590598  2.178357  1.474631e-01  0.854545  0.109091   \n",
       "37 -1.074117  0.775124 -0.582294  2.672183  1.797087e-01  0.290909  0.545455   \n",
       "38  1.568663 -0.682225 -0.040759 -0.500159  6.435698e-15  1.000000  0.000000   \n",
       "39 -0.366318 -0.684746  0.009482 -0.570258  0.000000e+00  1.000000  0.000000   \n",
       "40  1.419914  0.317944 -0.570726  2.506210  8.152693e-01  0.490909  0.290909   \n",
       "41 -1.594741  0.804143 -0.576197  2.456710  1.847307e-01  0.272727  0.509091   \n",
       "42  0.079929 -0.684726 -0.514242 -0.569895  2.426147e-09  1.000000  0.000000   \n",
       "43  1.568663 -0.681422 -0.228023 -0.477955  2.752713e-13  1.000000  0.000000   \n",
       "44 -0.031633  0.340540 -0.576024  1.828374  1.771941e-01  0.363636  0.090909   \n",
       "45  0.079929 -0.675405 -0.494609 -0.452266  4.425918e-05  1.000000  0.000000   \n",
       "46 -0.553495  1.751625 -0.689127 -0.295203  1.000000e+00  0.000000  1.000000   \n",
       "47 -0.031633 -0.611859 -0.574941  0.467159  2.321644e-04  0.963636  0.000000   \n",
       "48 -0.739432 -0.678477 -0.576024 -0.395924  3.201058e-14  1.000000  0.000000   \n",
       "49 -0.366318 -0.674336 -0.511830 -0.305414  1.875164e-07  1.000000  0.000000   \n",
       "50 -0.627870 -0.684746 -0.224566 -0.570258  0.000000e+00  1.000000  0.000000   \n",
       "51  1.568663 -0.682091  0.107113 -0.497764  2.924772e-11  1.000000  0.000000   \n",
       "52 -0.291944  1.755958 -0.753529 -0.430775  9.999962e-01  0.000000  1.000000   \n",
       "53  0.898052 -0.678945 -0.436401 -0.417220  1.200098e-05  1.000000  0.000000   \n",
       "54 -0.813806  0.570014 -0.576024  2.169265  8.225896e-01  0.163636  0.327273   \n",
       "55  0.377427 -0.682310 -0.399890 -0.518017  2.136237e-10  1.000000  0.000000   \n",
       "56  1.568663 -0.684746  0.519885 -0.570258  0.000000e+00  1.000000  0.000000   \n",
       "57  0.191491  1.763776 -1.072164 -0.550716  9.999995e-01  0.000000  1.000000   \n",
       "58  1.196791 -0.683025 -0.042193 -0.536262  1.021288e-10  1.000000  0.000000   \n",
       "59 -0.925368  0.118396 -0.573393  2.750627  6.916352e-01  0.581818  0.254545   \n",
       "\n",
       "    dual_std  dual_ema    dual_0    rc_std      rc_0  \n",
       "0  -0.664010  0.735678  1.421505  1.725125 -0.444962  \n",
       "1  -0.659058  0.715266  0.188488  0.014873 -0.444962  \n",
       "2  -0.406684  0.401936 -0.039848 -0.608326 -0.444962  \n",
       "3  -0.069888 -3.996999 -0.679190 -0.770071 -0.444962  \n",
       "4  -0.134165  0.524444 -0.359519  0.637014  3.579112  \n",
       "5  -0.477598  0.594859  0.690828  0.924593 -0.444962  \n",
       "6  -0.597154  0.735678  0.827830  0.730450 -0.444962  \n",
       "7  -0.642175  0.295497  0.234155  1.259253 -0.444962  \n",
       "8  -0.488375  0.734486  0.873497 -0.690961 -0.355538  \n",
       "9  -0.685796  0.076757  1.330170  1.785764 -0.444962  \n",
       "10  0.828594  0.724277 -0.359519 -0.297839  0.181005  \n",
       "11 -0.598531 -0.133278  1.056167  0.312410 -0.444962  \n",
       "12 -0.130091  0.385536  0.188488 -0.666495  0.270429  \n",
       "13 -0.259993  0.735596  0.142821 -0.281769 -0.444962  \n",
       "14 -0.221457 -0.346267 -0.176850  1.089641 -0.444962  \n",
       "15  2.235493  0.733158  0.234155 -0.805802 -0.444962  \n",
       "16  0.846804 -0.416943 -0.679190 -0.674427 -0.266114  \n",
       "17 -0.189325  0.731679  0.325490 -0.653983 -0.355538  \n",
       "18 -0.056810  0.268177  0.188488 -0.797343 -0.355538  \n",
       "19  0.139028 -1.401283 -0.587856  0.194598 -0.444962  \n",
       "20 -0.287491  0.706219  0.782163 -0.497475  0.181005  \n",
       "21 -0.734320  0.614111  1.147501  1.764267 -0.444962  \n",
       "22 -0.695045  0.150052  1.330170 -0.805802 -0.444962  \n",
       "23 -0.378471 -0.382737  1.010499  0.735819 -0.444962  \n",
       "24 -0.422657  0.075197  0.325490 -0.500047  0.002157  \n",
       "25 -0.100033 -2.417746 -1.044529 -0.545321 -0.444962  \n",
       "26  0.088485  0.130518 -0.587856  0.075257 -0.444962  \n",
       "27  0.104682 -2.297270 -1.044529 -0.708015 -0.444962  \n",
       "28 -0.366743  0.255881 -0.359519  1.102030 -0.444962  \n",
       "29 -0.517281 -0.369714 -0.222518 -0.470535  1.164668  \n",
       "30  0.863703 -0.528107 -0.222518 -0.679506 -0.355538  \n",
       "31  0.899096 -0.287204 -0.770525  0.242584 -0.444962  \n",
       "32  0.412787  0.238580 -0.724858 -0.303842 -0.355538  \n",
       "33 -0.512731 -0.286933  0.097154  1.428044 -0.444962  \n",
       "34 -0.741153  0.735675  1.330170  0.407375 -0.444962  \n",
       "35 -0.508248  0.320254  0.416825 -0.290214 -0.444962  \n",
       "36 -0.568528  0.735678  0.827830 -0.522143  0.091581  \n",
       "37 -0.290395 -0.863941 -0.176850 -0.680580 -0.176690  \n",
       "38 -0.444610  0.732704  0.051486  2.062016 -0.444962  \n",
       "39 -0.336217 -0.542050 -0.450854  0.675465 -0.444962  \n",
       "40  0.572834 -0.703202 -0.405187 -0.599089 -0.266114  \n",
       "41 -0.570279  0.594543  0.553826 -0.801433 -0.266114  \n",
       "42 -0.187780 -0.414871 -1.044529 -0.308557 -0.444962  \n",
       "43 -0.632404  0.735678  0.827830  1.769331 -0.444962  \n",
       "44 -0.255284 -0.915400 -0.542189 -0.805802 -0.444962  \n",
       "45 -0.549547 -0.624325 -0.039848 -0.306044 -0.444962  \n",
       "46 -0.158219  0.378628 -0.496521 -0.271886  3.757960  \n",
       "47 -0.020419 -2.042414 -0.542189 -0.773131 -0.444962  \n",
       "48 -0.590181  0.734015  0.325490 -0.805802 -0.444962  \n",
       "49 -0.371037 -0.505533 -0.176850 -0.085701 -0.444962  \n",
       "50 -0.628048  0.310364  1.010499  0.379427 -0.444962  \n",
       "51 -0.065142 -1.063512  0.553826  2.457615 -0.444962  \n",
       "52  0.572170 -2.313758  0.188488  0.079588  2.595450  \n",
       "53  1.377599 -0.441137 -0.861860  0.110516 -0.444962  \n",
       "54 -0.416136 -0.635528  0.416825 -0.805802 -0.444962  \n",
       "55 -0.017378 -1.772955 -0.633523  0.005773 -0.444962  \n",
       "56  1.222539  0.255655  1.010499  2.303833 -0.444962  \n",
       "57  0.275891 -1.815139  0.051486  0.947975  3.668536  \n",
       "58 -0.456782  0.148497  0.599494  1.524482 -0.444962  \n",
       "59 -0.301966 -1.808839  0.142821 -0.699456 -0.355538  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = ReadData(\"data/prediction.txt\")\n",
    "# print(df.shape)\n",
    "# print(df)\n",
    "# TrainModel(train, val, test)\n",
    "# Inference(df )\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-14T00:18:42.999397Z",
     "start_time": "2021-02-14T00:18:42.995284Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix , auc,roc_curve\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-14T00:18:43.360855Z",
     "start_time": "2021-02-14T00:18:43.358110Z"
    }
   },
   "outputs": [],
   "source": [
    "def ema(values):\n",
    "    ewm=0\n",
    "    for i in values:\n",
    "        ewm=(ewm+i)/2\n",
    "    return ewm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-14T00:13:40.763489Z",
     "start_time": "2021-02-14T00:13:40.760521Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-12-1a61888cf14f>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-1a61888cf14f>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    min fy + theta\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "min fy + theta\n",
    "uy-theta <=K\n",
    "\n",
    "y<=(K+theta)/u\n",
    "cap or buget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-14T00:18:46.905971Z",
     "start_time": "2021-02-14T00:18:46.902568Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data\n",
    "def ReadData():\n",
    "    # if changed order, update the predict.py\n",
    "    header=[\"LB\",\"UB\",\"ObjCoeff\",\"yLP\",\"Dual\",\"RC\",\"yInt\"]#\"LB\", \"ObjCoeff\",\"Dual\",\"RC\",\"y_lp_sum\",\"ylp\",\"yint\"]#\"RC\", \"ylp\", \"yint\"]#,\"Dual\",\"y_lp_sum\",\"LB\", \"Routing_Cost\",\"Capacity\",\"Fixed_Cost\"]\n",
    "    df = pd.concat([\n",
    "    #                 pd.read_csv(\"data/{}.csv\".format(\"NI\"))[col_list]\n",
    "                    pd.read_csv(\"data/{}.csv\".format(\"noRootLifter\"))[header]\n",
    "    ])#noRootLifter allSolsWithRC probSpecificScaling\n",
    "\n",
    "#     df[\"gap\"]=(df.UB-df.LB )/(1e-71+df.UB )\n",
    "    df.drop([\"LB\",\"UB\"],axis=\"columns\",inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-14T00:19:01.714108Z",
     "start_time": "2021-02-14T00:18:49.650177Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3698, 13)\n",
      "0    2158\n",
      "1    1540\n",
      "Name: yInt, dtype: int64\n",
      "y_0        -0.822939\n",
      "RC         -0.506116\n",
      "dual_0     -0.500042\n",
      "ObjCoeff   -0.262034\n",
      "dual_ema   -0.205534\n",
      "rc_std      0.215115\n",
      "dual_std    0.250149\n",
      "ylp_std     0.335374\n",
      "rc_0        0.674233\n",
      "y_1         0.757059\n",
      "yLP         0.835588\n",
      "ylp_ema     0.842173\n",
      "yInt        1.000000\n",
      "Name: yInt, dtype: float64\n",
      "Fractionality dum 498.2578087735849\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ObjCoeff</th>\n",
       "      <th>yLP</th>\n",
       "      <th>RC</th>\n",
       "      <th>yInt</th>\n",
       "      <th>ylp_std</th>\n",
       "      <th>ylp_ema</th>\n",
       "      <th>y_0</th>\n",
       "      <th>y_1</th>\n",
       "      <th>dual_std</th>\n",
       "      <th>dual_ema</th>\n",
       "      <th>dual_0</th>\n",
       "      <th>rc_std</th>\n",
       "      <th>rc_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002018</td>\n",
       "      <td>478.406331</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004196</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>957.2373</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.329296</td>\n",
       "      <td>0.002018</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004196</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ObjCoeff       yLP          RC  yInt   ylp_std   ylp_ema  y_0  y_1  \\\n",
       "0  1.000000  0.002018  478.406331     0  0.004196  0.000002  1.0  0.0   \n",
       "1  0.329296  0.002018    0.000000     0  0.004196  0.000002  1.0  0.0   \n",
       "\n",
       "   dual_std  dual_ema  dual_0    rc_std  rc_0  \n",
       "0       0.0       0.0     1.0  957.2373   0.0  \n",
       "1       0.0       0.0     1.0    0.0000   0.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "df= ReadData()\n",
    "# feature eng\n",
    "df[\"ylp_std\"]=0\n",
    "df[\"ylp_ema\"]=0\n",
    "df[\"y_0\"]=0\n",
    "df[\"y_1\"]=0\n",
    "\n",
    "df[\"dual_std\"]=0\n",
    "df[\"dual_ema\"]=0\n",
    "df[\"dual_0\"]=0\n",
    "\n",
    "df[\"rc_std\"]=0\n",
    "df[\"rc_0\"]=0\n",
    "# ACCUMULATION DISTRIBUTION LINE\n",
    "# df[\"adl\"]=0\n",
    "# df[\"aroonUp\"]=0\n",
    "# df[\"aroonDown\"]=0\n",
    "\n",
    "# ignore first t iterations\n",
    "t=5 \n",
    "for idx,row in df.iterrows(): \n",
    "    yRow=np.float_( row.yLP.split(\"_\")[t:-1])\n",
    "    dRow=np.float_( row.Dual.split(\"_\")[t:-1])\n",
    "    rcRow=np.float_( row.RC.split(\"_\")[t:-1])\n",
    "    #y\n",
    "    df.loc[idx,\"yLP\"] = yRow.mean()\n",
    "    df.loc[idx,\"ylp_ema\"] = ema(yRow)\n",
    "    df.loc[idx,\"ylp_std\"] = yRow.std()\n",
    "    df.loc[idx,\"y_0\"]=len([i for i in yRow if abs(i)<=0.25])/len(yRow)\n",
    "    df.loc[idx,\"y_1\"]=len([i for i in yRow if abs(i)>=0.75])/len(yRow)\n",
    "    #duals\n",
    "    df.loc[idx,\"Dual\"] = dRow.mean() \n",
    "    df.loc[idx,\"dual_ema\"] = ema(dRow)\n",
    "    df.loc[idx,\"dual_std\"] = dRow.std()\n",
    "    df.loc[idx,\"dual_0\"]=len([i for i in dRow if abs(i)<0.1])/len(dRow)\n",
    "    #RC    \n",
    "    df.loc[idx,\"RC\"] = rcRow.mean() \n",
    "    df.loc[idx,\"rc_std\"] = rcRow.std()\n",
    "    df.loc[idx,\"rc_0\"]=len([i for i in rcRow if (i)<0])/len(rcRow)\n",
    "    #     \n",
    "#     df.loc[idx,\"adl\"]=len(yRow)*((yRow[-1] - min(yRow)) - (max(yRow) - yRow[-1]))/(1e-10+max(yRow)-min(yRow))\n",
    "#     df.loc[idx,\"aroonUp\"]=(len(yRow) - np.argmax(yRow)-1)/(len(yRow))\n",
    "#     df.loc[idx,\"aroonDown\"]=(len(yRow) - np.argmin(yRow)-1)/(len(yRow))\n",
    " \n",
    "# df[\"ratio\"]=df[\"ObjCoeff\"]/(1+abs(df[\"Dual\"]))\n",
    "# df[\"adl\"]=   df[\"adl\"].diff()\n",
    "# set types\n",
    "col_list=[\"ObjCoeff\",\n",
    "#           \"gap\",\n",
    "          \"yLP\",\"ylp_ema\",\"ylp_std\",\"y_0\",\"y_1\",\n",
    "          \"Dual\",\"dual_ema\",\"dual_std\", \"dual_0\"\n",
    "          ,\"RC\",\"rc_std\",\"rc_0\"]\n",
    "df[col_list]=df[col_list].astype(\"float\")\n",
    "df[\"yInt\"]=df[\"yInt\"].fillna(0).astype(\"int\")\n",
    "df.drop([\"Dual\"], axis=\"columns\",inplace=True)\n",
    "# drop\n",
    "df.dropna(inplace=True)\n",
    "# info\n",
    "print(df.shape)\n",
    "print(df.yInt.value_counts()) \n",
    "print(df.corr().yInt.sort_values())\n",
    "print(\"Fractionality dum\",abs(df.yLP-df.yInt).sum())\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-14T00:19:15.352780Z",
     "start_time": "2021-02-14T00:19:15.346313Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# split and scale\n",
    "# df_=pd.concat([df,df[df.yInt==1].head(int(0.28*len(df)))])\n",
    "def Split(t=0.25,v=0.001):\n",
    "    train, test = train_test_split(pd.concat([df,df[df.yInt==1].head(\n",
    "        int(max(0,len(df[df.yInt==0])-len(df[df.yInt==1]))))]\n",
    "                                            ), test_size=t,random_state=42)\n",
    "    # print(100*train.yInt.value_counts(normalize=True))\n",
    "    print(100*test.yInt.value_counts(normalize=True))\n",
    "    train, val = train_test_split(train, test_size=v,random_state=42)\n",
    "    print(100*train.yInt.value_counts(normalize=True))\n",
    "    print(100*val.yInt.value_counts(normalize=True))\n",
    "\n",
    "    # ech problem should be scaled separately\n",
    "    for col in df.columns:\n",
    "    #     if  col==\"yint\" or col==\"ylp\" or col==\"y_lp_sum\" or col==\"y_0\" or col==\"y_1\" or col==\"ylp_ema\" or col==\"ylp_med\":\n",
    "    #         continue\n",
    "        if col==\"RC\"or col==\"rc_med\" or col==\"rc_std\" or col==\"rc_ema\" or col==\"Dual\" or col==\"dual_med\" or col==\"dual_std\" or col==\"dual_ema\" or col==\"d_c\":\n",
    "            scaler=StandardScaler().fit(train[[col]])\n",
    "            train[col]=scaler.transform(train[[col]])\n",
    "            test[col]=scaler.transform(test[[col]])\n",
    "            pickle.dump(scaler, open(\"scaler_{}.sav\".format(col),'wb'))\n",
    "    return train,test,val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-14T00:19:15.703852Z",
     "start_time": "2021-02-14T00:19:15.660652Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clfs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2d99ac6f84a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"yInt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mPrintFeatureImportance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-2d99ac6f84a4>\u001b[0m in \u001b[0;36mPrintFeatureImportance\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mPrintFeatureImportance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0ml\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"yInt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clfs' is not defined"
     ]
    }
   ],
   "source": [
    "# funcs\n",
    "def QuntileScore(df):\n",
    "    df[\"pred\"]=-1\n",
    "    df.loc[df.score>=df.score.quantile(0.75),\"pred\"]=1\n",
    "    df.loc[df.score<=df.score.quantile(0.25),\"pred\"]=0\n",
    "    print(len(df[df.pred!=-1])/len(df),\n",
    "          1-abs(df[df.pred!=-1].yInt-df[df.pred!=-1].pred).sum()/len(1e-10+df[df.pred!=-1]))\n",
    "\n",
    "def PrintStat(model):\n",
    "    trainScore=100*model.score(train.drop(\"yInt\",axis=\"columns\"), train.yInt)\n",
    "    testScore=100*model.score(test.drop(\"yInt\",axis=\"columns\"), test.yInt)\n",
    "    print(\"Score\",trainScore,testScore)\n",
    "\n",
    "    fpr_gb, tpr_gb, _ = roc_curve(test.yInt, model.decision_function(test.drop(\"yInt\",axis=\"columns\")))\n",
    "    fpr_gb_, tpr_gb_, _ = roc_curve(train.yInt, model.decision_function(train.drop(\"yInt\",axis=\"columns\")))\n",
    "    fpr_gb__, tpr_gb__, _ = roc_curve(df.yInt, model.decision_function(df.drop(\"yInt\",axis=\"columns\")))\n",
    "    print(\"AUC\",auc(fpr_gb_, tpr_gb_),auc(fpr_gb, tpr_gb), \"=>\",auc(fpr_gb__, tpr_gb__))\n",
    "    print(\"Quantiles\")\n",
    "    train[\"score\"]=model.predict_proba(train.drop(\"yInt\",axis=\"columns\"))[:,1]\n",
    "    QuntileScore(train)\n",
    "    test[\"score\"]=model.predict_proba(test.drop(\"yInt\",axis=\"columns\"))[:,1]\n",
    "    QuntileScore(test) \n",
    "    \n",
    "def PrintPermutationImportance():\n",
    "#     pass\n",
    "    # pickle.dump(clf, open(\"gboost.sav\", 'wb'))\n",
    "    r = permutation_importance(clfs, df.drop(\"yInt\",axis=\"columns\"), df.yInt\n",
    "                               ,n_repeats=100\n",
    "#                                , n_jobs=-1\n",
    "                               ,random_state=42\n",
    "                              )\n",
    "    for i in r.importances_mean.argsort()[::-1]:\n",
    "        print(np.round(r.importances_mean[i],5)\n",
    "#               r.importances_std[i],\n",
    "              ,r.importances_mean[i] - 2 * r.importances_std[i] >0\n",
    "              , df.drop('yInt',axis='columns').columns[i], \n",
    "             )\n",
    "def PrintFeatureImportance():\n",
    "    l=[]\n",
    "    for i in range(len(clfs.feature_importances_)):\n",
    "        l.append((clfs.feature_importances_[i], train.drop(\"yInt\",axis=\"columns\").columns[i]))\n",
    "    print(sorted(l))\n",
    "PrintFeatureImportance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-14T00:19:41.139558Z",
     "start_time": "2021-02-14T00:19:16.104369Z"
    },
    "cell_style": "center",
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    50.324374\n",
      "0    49.675626\n",
      "Name: yInt, dtype: float64\n",
      "0    50.077328\n",
      "1    49.922672\n",
      "Name: yInt, dtype: float64\n",
      "0    75.0\n",
      "1    25.0\n",
      "Name: yInt, dtype: float64\n",
      "Score 100.0 92.86376274328082\n",
      "AUC 1.0 0.9820923009263076 => 0.9784257910763935\n",
      "Quantiles\n",
      "0.5004639653572533 1.0\n",
      "0.5004633920296571 0.9944444444444445\n"
     ]
    }
   ],
   "source": [
    "# boost\n",
    "train,test,val=Split()\n",
    "clfs = GradientBoostingClassifier(random_state=42\n",
    "                                ,learning_rate=0.02\n",
    "                                 ,n_estimators=1500\n",
    "                                 ,max_depth=8\n",
    "#                                  ,min_samples_split=0.9\n",
    "#                                  ,subsample=0.7\n",
    "                                ).fit(train.drop(\"yInt\",axis=\"columns\"), train.yInt)\n",
    "PrintStat(clfs)\n",
    "# PrintPermutationImportance()\n",
    "# PrintFeatureImportance()\n",
    "\n",
    "# AUC 1.0 0.9836659245210412 => 0.9801085962230541\n",
    "# Quantiles\n",
    "# 0.5004639653572533 1.0\n",
    "# 0.5004633920296571 0.9962962962962963"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-14T00:19:41.246633Z",
     "start_time": "2021-02-14T00:19:41.141778Z"
    }
   },
   "outputs": [],
   "source": [
    "pickle.dump(clfs, open(\"data/GradientBoostingClassifier.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-31T23:55:11.275819Z",
     "start_time": "2021-01-31T23:55:11.034308Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0029390546637066708, 'rc_0'),\n",
       " (0.005706651675163769, 'y_1'),\n",
       " (0.013154671377476328, 'Dual'),\n",
       " (0.01386702375833122, 'dual_0'),\n",
       " (0.015286930405662298, 'rc_std'),\n",
       " (0.017316468504730627, 'dual_ema'),\n",
       " (0.0180704659684177, 'ratio'),\n",
       " (0.021242659073833894, 'RC'),\n",
       " (0.02220925941501988, 'adl'),\n",
       " (0.023067397952430874, 'ylp_std'),\n",
       " (0.023791644050929477, 'ObjCoeff'),\n",
       " (0.024661693514274993, 'gap'),\n",
       " (0.028930754045545976, 'dual_std'),\n",
       " (0.06437783276086707, 'yLP'),\n",
       " (0.11134714954143661, 'ylp_ema'),\n",
       " (0.5940303432921726, 'y_0')]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-31T23:08:15.830699Z",
     "start_time": "2021-01-31T23:07:48.038312Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False  True False False False False  True  True False False False False\n",
      " False False False False]\n",
      "Index(['yLP', 'ylp_ema', 'y_0'], dtype='object')\n",
      "total features: 16\n",
      "selected features: 3\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'GradientBoostingClassifier' object has no attribute 'coef_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-4e23186cceb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'selected features: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected_feat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m print('features with coefficients shrank to zero: {}'.format(\n\u001b[0;32m---> 10\u001b[0;31m       np.sum(sel_.estimator_.coef_ == 0)))\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mremoved_feats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"yInt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msel_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mremoved_feats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GradientBoostingClassifier' object has no attribute 'coef_'"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "sel_ = SelectFromModel(clfs)\n",
    "sel_.fit(train.drop(\"yInt\",axis=\"columns\"), train.yInt)\n",
    "print(sel_.get_support())\n",
    "selected_feat = train.drop(\"yInt\",axis=\"columns\").columns[(sel_.get_support())]\n",
    "print(selected_feat)\n",
    "print('total features: {}'.format((train.drop(\"yInt\",axis=\"columns\").shape[1])))\n",
    "print('selected features: {}'.format(len(selected_feat)))\n",
    "print('features with coefficients shrank to zero: {}'.format(\n",
    "      np.sum(sel_.estimator_.coef_ == 0)))\n",
    "removed_feats = train.drop(\"yInt\",axis=\"columns\").columns[(sel_.estimator_.coef_ == 0).ravel().tolist()]\n",
    "removed_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T01:51:20.229174Z",
     "start_time": "2021-01-11T01:51:09.541346Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    51.898734\n",
      "0    48.101266\n",
      "Name: yInt, dtype: float64\n",
      "0    50.635055\n",
      "1    49.364945\n",
      "Name: yInt, dtype: float64\n",
      "1    50.0\n",
      "0    50.0\n",
      "Name: yInt, dtype: float64\n",
      "95.08890770533446\n",
      "90.63291139240506\n",
      "yLP      0.184 +/- 0.018 True\n",
      "ylp_std  0.157 +/- 0.018 True\n",
      "RC       0.079 +/- 0.013 True\n",
      "rc_std   0.069 +/- 0.013 True\n",
      "Dual     0.036 +/- 0.011 True\n",
      "y_1      0.022 +/- 0.007 True\n",
      "yInt     0.016 +/- 0.006 True\n",
      "dual_ema 0.014 +/- 0.009 False\n",
      "y_0      0.011 +/- 0.008 False\n",
      "dual_std 0.011 +/- 0.009 False\n",
      "dual_0   0.003 +/- 0.007 False\n",
      "ObjCoeff 0.003 +/- 0.008 False\n",
      "ylp_ema  -0.013 +/- 0.006 False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(max_iter=100000, random_state=1, tol=1e-27)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mlp\n",
    "train,test,val=Split(v=0.001)\n",
    "mlp = MLPClassifier(\n",
    "#     hidden_layer_sizes=(5000, 500)\n",
    "    random_state=1,max_iter=100000\n",
    "    , tol=1e-27\n",
    ").fit(    train.drop(\"yInt\",axis=\"columns\"), train.yInt)\n",
    "# solver='lbfgs', alpha=1e-5,  hidden_layer_sizes=(50, 20), random_state=1,max_iter=1000\n",
    "print(100*mlp.score(train.drop(\"yInt\",axis=\"columns\"), train.yInt))\n",
    "print(100*mlp.score(test.drop(\"yInt\",axis=\"columns\"), test.yInt))\n",
    "pickle.dump(mlp, open(\"mlp.sav\", 'wb'))\n",
    "\n",
    "\n",
    "r = permutation_importance(mlp, test.drop(\"yInt\",axis=\"columns\"), test.yInt, n_repeats=100, random_state=0)\n",
    "for i in r.importances_mean.argsort()[::-1]:\n",
    "    print( f\"{test.columns[i]:<8}\" f\" {r.importances_mean[i]:.3f}\" f\" +/- {r.importances_std[i]:.3f}\" f\" {r.importances_mean[i] - 2 * r.importances_std[i] > 0}\")\n",
    "\n",
    "mlp\n",
    "# 0.9564102564102565\n",
    "# 0.9387755102040817\n",
    "# ylp      0.349 +/- 0.046 True\n",
    "# RC       0.055 +/- 0.022 True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T00:34:16.378635Z",
     "start_time": "2021-01-11T00:34:13.874522Z"
    },
    "cell_style": "center",
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9354250273622765\n",
      "0.9445048966267682\n",
      "ylp_std  0.042 +/- 0.006 True\n",
      "RC       0.031 +/- 0.006 True\n",
      "ylp_ema  0.016 +/- 0.004 True\n",
      "yLP      0.013 +/- 0.005 True\n",
      "y_0      0.006 +/- 0.003 True\n",
      "dual_ema 0.003 +/- 0.002 False\n",
      "ObjCoeff 0.002 +/- 0.002 False\n",
      "y_1      0.002 +/- 0.003 False\n",
      "dual_std 0.002 +/- 0.001 False\n",
      "rc_std   0.002 +/- 0.001 False\n",
      "Dual     0.001 +/- 0.001 False\n",
      "dual_0   0.000 +/- 0.000 False\n",
      "yInt     0.000 +/- 0.000 False\n",
      "Feature: ObjCoeff, Score: -0.8756462189364983\n",
      "Feature: yLP, Score: 1.8175542942172593\n",
      "Feature: Dual, Score: 0.07044974955302002\n",
      "Feature: RC, Score: -1.7981189873471957\n",
      "Feature: yInt, Score: 0.13341485770808634\n",
      "Feature: ylp_std, Score: 2.4920701474674116\n",
      "Feature: ylp_ema, Score: -1.9700185333096643\n",
      "Feature: y_0, Score: 1.0727496438154067\n",
      "Feature: y_1, Score: 0.5189935188374641\n",
      "Feature: dual_std, Score: 0.1647333254734069\n",
      "Feature: dual_ema, Score: -0.37249080665560647\n",
      "Feature: dual_0, Score: 0.011240154914805317\n",
      "Feature: rc_std, Score: 0.5366643571640523\n"
     ]
    }
   ],
   "source": [
    "# logistic\n",
    "logstic = LogisticRegression(max_iter=10000, tol=1e-10)#make_pipeline(StandardScaler(), LogisticRegression())\n",
    "logstic.fit(train.drop(\"yInt\",axis=\"columns\"), train.yInt) \n",
    "\n",
    "print(logstic.score(train.drop(\"yInt\",axis=\"columns\"), train.yInt))\n",
    "print(logstic.score(test.drop(\"yInt\",axis=\"columns\"), test.yInt))\n",
    "pickle.dump(logstic, open(\"logistic.sav\", 'wb'))\n",
    "\n",
    "r = permutation_importance(logstic, test.drop(\"yInt\",axis=\"columns\"), test.yInt, n_repeats=100, random_state=0)\n",
    "for i in r.importances_mean.argsort()[::-1]:\n",
    "    print( f\"{test.columns[i]:<8}\" f\" {r.importances_mean[i]:.3f}\" f\" +/- {r.importances_std[i]:.3f}\" f\" {r.importances_mean[i] - 2 * r.importances_std[i] > 0}\")\n",
    "importance = logstic.coef_[0]\n",
    "for i,v in enumerate(importance):\n",
    "    print('Feature: {}, Score: {}'.format (test.columns[i],v))\n",
    "    # 0.9538461538461539\n",
    "# 0.9387755102040817\n",
    "\n",
    "# 0.9140625\n",
    "# 0.9166666666666666\n",
    "\n",
    "# 0.9157986111111112\n",
    "# 0.9131944444444444\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T00:34:31.410220Z",
     "start_time": "2021-01-11T00:34:16.382034Z"
    },
    "cell_style": "center",
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9332360452389639\n",
      "0.9423286180631121\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-151-e29a00bde14d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvm_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"svm.sav\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpermutation_importance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvm_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"yInt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myInt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_repeats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimportances_mean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34mf\"{test.columns[i]:<8}\"\u001b[0m \u001b[0;34mf\" {r.importances_mean[i]:.3f}\"\u001b[0m \u001b[0;34mf\" +/- {r.importances_std[i]:.3f}\"\u001b[0m \u001b[0;34mf\" {r.importances_mean[i] - 2 * r.importances_std[i] > 0}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/3.8/site-packages/sklearn/inspection/_permutation_importance.py\u001b[0m in \u001b[0;36mpermutation_importance\u001b[0;34m(estimator, X, y, scoring, n_repeats, n_jobs, random_state)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0mbaseline_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m     scores = Parallel(n_jobs=n_jobs)(delayed(_calculate_permutation_scores)(\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_repeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     ) for col_idx in range(X.shape[1]))\n",
      "\u001b[0;32m/Library/Python/3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/3.8/site-packages/sklearn/inspection/_permutation_importance.py\u001b[0m in \u001b[0;36m_calculate_permutation_scores\u001b[0;34m(estimator, X, y, col_idx, random_state, n_repeats, scorer)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mX_permuted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_permuted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mshuffling_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mfeature_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_permuted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_round\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/3.8/site-packages/sklearn/metrics/_scorer.py\u001b[0m in \u001b[0;36m_passthrough_scorer\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_passthrough_scorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;34m\"\"\"Function that wraps estimator.score\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    497\u001b[0m         \"\"\"\n\u001b[1;32m    498\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_more_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/3.8/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    613\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/3.8/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_for_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse_predict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dense_predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_dense_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/3.8/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_dense_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0msvm_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLIBSVM_IMPL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_impl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         return libsvm.predict(\n\u001b[0m\u001b[1;32m    353\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupport_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupport_vectors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_support\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dual_coef_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_intercept_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# svm\n",
    "svm_ = svm.SVC()#make_pipeline(StandardScaler(), svm.SVC())\n",
    "svm_.fit(train.drop(\"yInt\",axis=\"columns\"), train.yInt) \n",
    "\n",
    "print(svm_.score(train.drop(\"yInt\",axis=\"columns\"), train.yInt))\n",
    "print(svm_.score(test.drop(\"yInt\",axis=\"columns\"), test.yInt))\n",
    "pickle.dump(svm_, open(\"svm.sav\", 'wb'))\n",
    "\n",
    "r = permutation_importance(svm_, test.drop(\"yInt\",axis=\"columns\"), test.yInt, n_repeats=100, random_state=0)\n",
    "for i in r.importances_mean.argsort()[::-1]:\n",
    "    print( f\"{test.columns[i]:<8}\" f\" {r.importances_mean[i]:.3f}\" f\" +/- {r.importances_std[i]:.3f}\" f\" {r.importances_mean[i] - 2 * r.importances_std[i] > 0}\")\n",
    "# 0.9560117302052786\n",
    "# 0.9523809523809523\n",
    "\n",
    "# 0.9262152777777778\n",
    "# 0.9097222222222222\n",
    "\n",
    "# 0.9236111111111112\n",
    "# 0.9131944444444444\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T00:34:31.413002Z",
     "start_time": "2021-01-11T00:34:04.125Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(50, 20),(500, 20), (50,200),(50,50,50), (50,100,50), (100,)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam','lbfgs'],\n",
    "    'alpha': [0.0001, 0.05, 1e-5, 0.1],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}\n",
    "mlpcv = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=3)\n",
    "mlpcv.fit(train.drop(\"yint\",axis=\"columns\"), train.yint)\n",
    "# Best paramete set\n",
    "print('Best parameters found:\\n', mlpcv.best_params_)\n",
    "# All results\n",
    "means = mlpcv.cv_results_['mean_test_score']\n",
    "stds = mlpcv.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, mlpcv.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T00:34:31.414704Z",
     "start_time": "2021-01-11T00:34:04.536Z"
    },
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "df_=pd.read_csv(\"ml_output.csv\")[[\"Reduced_Cost\",\"y_lp\"]]\n",
    "print(clf.score(df_, df_.y_lp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T00:34:31.416104Z",
     "start_time": "2021-01-11T00:34:04.637Z"
    },
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(250,10))\n",
    "plt.scatter(range(len(test[\"y\"])),test[\"y\"])\n",
    "plt.scatter(range(len(test[\"y\"])),test[\"y_int\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T00:34:31.417781Z",
     "start_time": "2021-01-11T00:34:04.790Z"
    }
   },
   "outputs": [],
   "source": [
    "param_grid = {'C': [0.1, 1, 10, 100, 1000],  \n",
    "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n",
    "              'kernel': ['rbf']}  \n",
    "  \n",
    "grid = GridSearchCV(svm.SVC(), param_grid, refit = True, verbose = 3) \n",
    "  \n",
    "# fitting the model for grid search \n",
    "grid.fit(train.drop(\"y_int\",axis=\"columns\"), train.y_int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T00:34:31.418844Z",
     "start_time": "2021-01-11T00:34:04.872Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test[\"score\"]=pipe.decision_function(test.drop([\"y_int\"],axis=\"columns\"))\n",
    "# test[\"pred\"]=pipe.predict(test.drop([\"score\",\"y_int\"],axis=\"columns\"))\n",
    "# print(100*len(test[test.y!=test.y_int])/len(test))\n",
    "# print(test[test.y!=test.y_int][[\"y_int\",\"y\"]].sort_values(\"y\").head(1000))\n",
    "print(test[[\"y_lp\",\"y_int\",\"score\",\"pred\"]].describe())\n",
    "test[[\"y_lp\",\"y_int\",\"score\",\"pred\"]].sort_values(\"score\")[test.pred!=test.y_int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T00:34:31.421572Z",
     "start_time": "2021-01-11T00:34:05.046Z"
    }
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "#     \"loss\":[\"deviance\"],\n",
    "    \"learning_rate\": [0.001,0.01, 0.025, 0.05, 0.075, 0.1],\n",
    "#     \"min_samples_split\": np.linspace(0.1, 0.5, 12),\n",
    "#     \"min_samples_leaf\": np.linspace(0.1, 0.5, 12),\n",
    "    \"max_depth\":[3,7,18, 30],\n",
    "#     \"max_features\":[\"log2\",\"sqrt\"],\n",
    "#     \"criterion\": [\"friedman_mse\",  \"mae\"],\n",
    "#     \"subsample\":[0.5, 0.618, 0.8, 0.85, 0.9, 0.95, 1.0],\n",
    "    \"n_estimators\":[10, 200, 500, 1000, 3000, 5000, 10000]\n",
    "    }\n",
    "clf = GridSearchCV(GradientBoostingClassifier(), parameters, cv=3, n_jobs=-1,verbose=100)\n",
    "\n",
    "clf.fit(df.drop(\"yInt\",axis=\"columns\"), df.yInt)\n",
    "print(clf.score(df.drop(\"yInt\",axis=\"columns\"), df.yInt))\n",
    "print(clf.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T01:28:12.135616Z",
     "start_time": "2021-01-11T01:28:02.174949Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    51.898734\n",
      "0    48.101266\n",
      "Name: yInt, dtype: float64\n",
      "0    51.634724\n",
      "1    48.365276\n",
      "Name: yInt, dtype: float64\n",
      "1    52.364865\n",
      "0    47.635135\n",
      "Name: yInt, dtype: float64\n",
      "3 887 0.06878962790622982\n",
      "SCORE                                100.0 86.82432432432432\n",
      "6 4599 0.10346436654968102\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-260-ac207c653955>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.00001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     clf = GradientBoostingClassifier(random_state=42,\n\u001b[0m\u001b[1;32m      9\u001b[0m                                 \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                                  \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1500\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/3.8/site-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0;31m# fit the boosting stages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         n_stages = self._fit_stages(\n\u001b[0m\u001b[1;32m    499\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m             sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\n",
      "\u001b[0;32m/Library/Python/3.8/site-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[0;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m                 \u001b[0;31m# no need to fancy index w/ no subsampling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_score_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/3.8/site-packages/sklearn/ensemble/_gb_losses.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, y, raw_predictions, sample_weight)\u001b[0m\n\u001b[1;32m    606\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             return (-2 / sample_weight.sum() * np.sum(\n\u001b[0;32m--> 608\u001b[0;31m                 sample_weight * ((y * raw_predictions) -\n\u001b[0m\u001b[1;32m    609\u001b[0m                                  np.logaddexp(0, raw_predictions))))\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# boost\n",
    "train,test,val=Split(v=0.25)\n",
    "for d in range(1000):#range(7,30):\n",
    "    d=np.random.randint(low=3,high=7)\n",
    "    t=np.random.randint(100,5000)\n",
    "    r=2*np.random.rand()/10 + 0.00001\n",
    "    print(d,t,r)\n",
    "    clf = GradientBoostingClassifier(random_state=42,\n",
    "                                learning_rate=0.025,\n",
    "                                 n_estimators=1500\n",
    "                                 ,max_depth=5\n",
    "                                ).fit(train.drop(\"yInt\",axis=\"columns\"), train.yInt)\n",
    "    trainScore=clf.score(train.drop(\"yInt\",axis=\"columns\"), train.yInt)\n",
    "    testScore=clf.score(val.drop(\"yInt\",axis=\"columns\"), val.yInt)\n",
    "    print(\"SCORE                               \",100*trainScore,100*testScore)\n",
    "    trainScores.append(trainScore)\n",
    "    testScores.append(testScore)\n",
    "#     if trainScore>0.999:\n",
    "#         for t in range(100,100000,250):\n",
    "#             clf = GradientBoostingClassifier(random_state=42,\n",
    "#         #                                 learning_rate=0.01,\n",
    "#                                          n_estimators=t,\n",
    "#                                          max_depth=d\n",
    "#                                         ).fit(train.drop(\"yInt\",axis=\"columns\"), train.yInt)\n",
    "#             trainScore=clf.score(train.drop(\"yInt\",axis=\"columns\"), train.yInt)\n",
    "#             testScore=clf.score(val.drop(\"yInt\",axis=\"columns\"), val.yInt)\n",
    "#             print(d,t,trainScore,testScore)\n",
    "#             trainScores.append(trainScore)\n",
    "#             testScores.append(testScore)\n",
    "#             if testScore>0.99:\n",
    "#                 break\n",
    "#         break\n",
    "\n",
    "# pickle.dump(clf, open(\"gboost.sav\", 'wb'))\n",
    "\n",
    "# r = permutation_importance(clf, test.drop(\"yint\",axis=\"columns\"), test.yint, n_repeats=100, random_state=0)\n",
    "# for i in r.importances_mean.argsort()[::-1]:\n",
    "#     print( f\"{test.columns[i]:<8}\" f\" {r.importances_mean[i]:.3f}\" f\" +/- {r.importances_std[i]:.3f}\" f\" {r.importances_mean[i] - 2 * r.importances_std[i] > 0}\")\n",
    "\n",
    "\n",
    "# 0.9208416833667334\n",
    "# 0.9151636606546426\n",
    "# ylp      0.392 +/- 0.008 True\n",
    "# y_lp_sum 0.086 +/- 0.005 True\n",
    "# ObjCoeff 0.011 +/- 0.002 True\n",
    "# RC       0.004 +/- 0.002 False\n",
    "# Dual     0.002 +/- 0.002 False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T20:51:33.615395Z",
     "start_time": "2021-02-13T20:51:08.327554Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "No output_directory specified. Models will be saved in: AutogluonModels/ag-20210213_205108/\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to AutogluonModels/ag-20210213_205108/\n",
      "AutoGluon Version:  0.0.15\n",
      "Train Data Rows:    3233\n",
      "Train Data Columns: 14\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n",
      "\t2 unique label values:  [1, 0]\n",
      "\tIf 'binary' is not the correct problem_type, please manually specify the problem_type argument in fit() (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    5213.33 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.39 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tUseless Original Features (Count: 3): ['y_1', 'dual_std', 'rc_0']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 11 | ['ObjCoeff', 'yLP', 'RC', 'gap', 'ylp_std', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 11 | ['ObjCoeff', 'yLP', 'RC', 'gap', 'ylp_std', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t11 features in original data used to generate 11 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.31 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.07s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric argument of fit()\n",
      "AutoGluon will early stop models using evaluation metric: 'accuracy'\n",
      "Fitting model: RandomForestClassifierGini ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    50.324374\n",
      "0    49.675626\n",
      "Name: yInt, dtype: float64\n",
      "0    50.077328\n",
      "1    49.922672\n",
      "Name: yInt, dtype: float64\n",
      "0    75.0\n",
      "1    25.0\n",
      "Name: yInt, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.892\t = Validation accuracy score\n",
      "\t0.62s\t = Training runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestClassifierEntr ...\n",
      "\t0.902\t = Validation accuracy score\n",
      "\t0.69s\t = Training runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: ExtraTreesClassifierGini ...\n",
      "\t0.91\t = Validation accuracy score\n",
      "\t0.54s\t = Training runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: ExtraTreesClassifierEntr ...\n",
      "\t0.908\t = Validation accuracy score\n",
      "\t0.48s\t = Training runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: KNeighborsClassifierUnif ...\n",
      "\t0.866\t = Validation accuracy score\n",
      "\t0.0s\t = Training runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: KNeighborsClassifierDist ...\n",
      "\t0.892\t = Validation accuracy score\n",
      "\t0.0s\t = Training runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMClassifier ...\n",
      "\t0.908\t = Validation accuracy score\n",
      "\t0.45s\t = Training runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMClassifierXT ...\n",
      "\t0.904\t = Validation accuracy score\n",
      "\t0.5s\t = Training runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: CatboostClassifier ...\n",
      "\t0.91\t = Validation accuracy score\n",
      "\t1.69s\t = Training runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetClassifier ...\n",
      "\t0.89\t = Validation accuracy score\n",
      "\t17.26s\t = Training runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMClassifierCustom ...\n",
      "\t0.924\t = Validation accuracy score\n",
      "\t0.69s\t = Training runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: weighted_ensemble_k0_l1 ...\n",
      "\t0.926\t = Validation accuracy score\n",
      "\t0.48s\t = Training runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 24.6s ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         model  score_test  score_val  pred_time_test  pred_time_val   fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0     ExtraTreesClassifierGini    0.932345      0.910        0.122884       0.059099   0.539061                 0.122884                0.059099           0.539061            0       True          3\n",
      "1   RandomForestClassifierGini    0.931418      0.892        0.077367       0.046865   0.615660                 0.077367                0.046865           0.615660            0       True          1\n",
      "2     ExtraTreesClassifierEntr    0.931418      0.908        0.138786       0.059022   0.481624                 0.138786                0.059022           0.481624            0       True          4\n",
      "3   RandomForestClassifierEntr    0.930491      0.902        0.084754       0.057819   0.685796                 0.084754                0.057819           0.685796            0       True          2\n",
      "4           LightGBMClassifier    0.926784      0.908        0.006270       0.003753   0.446545                 0.006270                0.003753           0.446545            0       True          7\n",
      "5           CatboostClassifier    0.925857      0.910        0.009197       0.003921   1.693117                 0.009197                0.003921           1.693117            0       True          9\n",
      "6         LightGBMClassifierXT    0.924930      0.904        0.011316       0.004005   0.500152                 0.011316                0.004005           0.500152            0       True          8\n",
      "7      weighted_ensemble_k0_l1    0.924004      0.926        0.019565       0.008842   1.621056                 0.003226                0.001675           0.479805            1       True         12\n",
      "8     LightGBMClassifierCustom    0.923077      0.924        0.010069       0.003414   0.694706                 0.010069                0.003414           0.694706            0       True         11\n",
      "9     KNeighborsClassifierDist    0.923077      0.892        0.020805       0.016973   0.004376                 0.020805                0.016973           0.004376            0       True          6\n",
      "10         NeuralNetClassifier    0.919370      0.890        0.050877       0.021914  17.264725                 0.050877                0.021914          17.264725            0       True         10\n",
      "11    KNeighborsClassifierUnif    0.914736      0.866        0.014508       0.010332   0.004148                 0.014508                0.010332           0.004148            0       True          5\n"
     ]
    }
   ],
   "source": [
    "import autogluon as ag\n",
    "from autogluon import TabularPrediction as task\n",
    "\n",
    "# train_data = TabularDataset('https://autogluon.s3.amazonaws.com/datasets/Inc/train.csv')\n",
    "# test_data = TabularDataset('https://autogluon.s3.amazonaws.com/datasets/Inc/test.csv')\n",
    "train,test,val=Split()\n",
    "predictor = task.fit(train,label='yInt')  # Fit models for 60s\n",
    "leaderboard = predictor.leaderboard(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
